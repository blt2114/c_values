{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import  datetime\n",
    "\n",
    "import pyproj\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gpytorch\n",
    "import zipfile\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "import affine_operator_win_bounds as affine_ops "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP for Carthe DATA\n",
    "\n",
    "### For each velocity component\n",
    "* for comparing drifters $i$ and $j$ at times $k$ and $l$.\n",
    "\n",
    "$$\n",
    "k(u_{i,k}, u_{j,l}) = \\sigma_1^2 \\exp\\left\\{-\\frac{1}{2}\\left[\n",
    "    \\frac{\\|x_{i,k} - x_{j,k}\\|^2}{r_{s1}^2}  +\n",
    "    \\frac{(t_{i,k} - t_{j,k})^2}{r_{t1}^2} \n",
    "\\right] \\right\\} + \n",
    "\\sigma_2^2 \\exp\\left\\{-\\frac{1}{2}\\left[\n",
    "    \\frac{\\|x_{i,k} - x_{j,k}\\|^2}{r_{s2}^2}  +\n",
    "    \\frac{(t_{i,k} - t_{j,k})^2}{r_{t2}^2} \n",
    "\\right] \\right\\} + \\mathbb{I}[(i,l)=(j,k)]\\sigma_N^2\n",
    "$$\n",
    "\n",
    "* Here $\\sigma_1^2$ and $\\sigma_2^2$ are signal variances,\n",
    "$r_{s1}$ and $r_{s2}$ are spatial length scales and \n",
    "$r_{t1}$ and $r_{t2}$ are temporal length scales.\n",
    "* This constrains the length scales to be the same for both lattitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data from https://data.gulfresearchinitiative.org/data/R1.x134.073:0004\n",
    "data_dir = \"../data/oceans/GLAD_15min_filtered\"\n",
    "if not os.path.isdir(data_dir):\n",
    "    with zipfile.ZipFile(data_dir +\".zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "fn = data_dir + \"/GLAD_15min_filtered.dat\"\n",
    "df_full = pd.read_csv(fn, delim_whitespace=True, header=None, skiprows=5)\n",
    "\n",
    "df_full = df_full.rename(columns={0:\"drifter\", 1:\"date\", 2:\"time\", 3:\"Latitude\", 4:\"Longitude\",\n",
    "                  5:\"Pos Error\", 6:\"U\", 7:\"V\", 8:\"Vel Error\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add columns corresponding to time in hours and position in Kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline:  2012-07-20 01:15:00.143960\n",
      "len(all_times_hours): 1602883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briantrippe/anaconda/lib/python3.6/site-packages/pyproj/crs/crs.py:280: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  projstring = _prepare_from_string(projparams)\n"
     ]
    }
   ],
   "source": [
    "# First add time in hours\n",
    "fmt  = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "baseline_str = df_full.iloc[0].date  + \" \"+ df_full.iloc[0].time\n",
    "baseline = datetime.strptime(baseline_str, fmt)\n",
    "print(\"baseline: \", baseline)\n",
    "def time_from_baseline(baseline, date, time):\n",
    "    dt_str = date + \" \" + time\n",
    "    dt = datetime.strptime(dt_str, fmt)\n",
    "    return (dt - baseline).total_seconds()\n",
    "\n",
    "# add time in hours attribute\n",
    "all_dates, all_times = df_full.date, df_full.time\n",
    "all_times_seconds = []\n",
    "for date, time in zip(all_dates, all_times):\n",
    "    all_times_seconds.append(time_from_baseline(baseline, date, time))\n",
    "all_times_hours = np.array(all_times_seconds)/60/60\n",
    "print(\"len(all_times_hours):\", len(all_times_hours))\n",
    "\n",
    "df_full['hour'] = all_times_hours\n",
    "\n",
    "\n",
    "# Next add location in kilometers, taken from John Lodise's code\n",
    "################################Set up Coordinate system #################################\n",
    "lat0 = 28.2\n",
    "lon0 = -88.35\n",
    "NAD83 = pyproj.Proj(\"+init=EPSG:3453\", preserve_units = False)#False = meters #Louisiana South (ftUS)\n",
    "x_ori, y_ori = NAD83(lon0,lat0) #define x,y origin using lat0,lon0\n",
    "\n",
    "# Set limits in space (both in lat/lon and Km from origin) to be used for later filtering.\n",
    "lat_min, lat_max = 26., 29.5\n",
    "lon_min, lon_max = -89.5, -85.\n",
    "(x_min, y_min), (x_max, y_max) = NAD83(lon_min, lat_min), NAD83(lon_max, lat_max)\n",
    "x_min, x_max, y_min, y_max = (x_min-x_ori)/1000, (x_max-x_ori)/1000, (y_min-y_ori)/1000, (y_max-y_ori)/1000\n",
    "\n",
    "lons, lats = df_full.Longitude, df_full.Latitude\n",
    "x, y = NAD83(lons, lats)\n",
    "# Center around origin and scale to Kilometers\n",
    "x, y = (x - x_ori)/1000, (y - y_ori)/1000\n",
    "df_full['x'], df_full['y'] = x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the data to more manageable size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subset_data(df_full, t_min, t_max, lat_min, lat_max, lon_min, lon_max, downsample_freq, n_drifters):\n",
    "    df = df_full.copy()\n",
    "    \n",
    "    # filter to fewer hours \n",
    "    df = df[df.hour >=t_min]\n",
    "    df = df[df.hour <=t_max]\n",
    "\n",
    "    # filter to smaller spatial area\n",
    "    df = df[df.Latitude >= lat_min]\n",
    "    df = df[df.Latitude <= lat_max]\n",
    "    df = df[df.Longitude >= lon_min]\n",
    "    df = df[df.Longitude <= lon_max]\n",
    "\n",
    "    # filter to first 10 random drifters\n",
    "    drifter_ids = df.drifter.unique()\n",
    "    np.random.seed(42)\n",
    "    drifter_set = np.random.choice(drifter_ids, replace=False, size=n_drifters)\n",
    "    df = df[df.drifter.isin(drifter_set)]\n",
    "\n",
    "    # downsample time \n",
    "    if not \"index_2\" in df.columns:\n",
    "        df['index_2'] = np.array(range(len(df)))\n",
    "        df = df[df.index_2%downsample_freq == 0 ]\n",
    "\n",
    "    print(\"number of rows: \", len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull out a subset of the data on which to select kernel parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  3430\n"
     ]
    }
   ],
   "source": [
    "# start at t = 424 hours,  for evaluation data we will start at 400\n",
    "t_min, t_max  = 424, 900 \n",
    "downsample_freq = 10\n",
    "n_drifters = 20\n",
    "\n",
    "df = subset_data(df_full, t_min=t_min, t_max=t_max, lat_min=lat_min, lat_max=lat_max, lon_min=lon_min,\n",
    "                lon_max=lon_max, downsample_freq=downsample_freq, n_drifters=n_drifters)\n",
    "\n",
    "# pull out covariates (time, lat, long) and responses (U, V)\n",
    "X, Y = df[['hour', 'x','y']].to_numpy(), df['U'].to_numpy()\n",
    "X = torch.tensor(X, dtype=torch.float)\n",
    "Y = torch.tensor(Y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GPyTorch Gaussian Process Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class TwoScaleGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, cut_long_ls=False, cut_short_ls=False):\n",
    "        \"\"\"train_x is shape [N, 3], each x[0] is [t, lat, lon]\n",
    "        \"\"\"\n",
    "        super(TwoScaleGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(active_dims=[0])*\n",
    "            gpytorch.kernels.RBFKernel(active_dims=[1,2])\n",
    "        ) + gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(active_dims=[0])*\n",
    "            gpytorch.kernels.RBFKernel(active_dims=[1,2])\n",
    "        )\n",
    "        self.heuristic_init()\n",
    "        #k1 (scale: 0.06, ls: 5.2, 17.5)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001297\n",
    "\n",
    "        \n",
    "    def heuristic_init(self):\n",
    "        ### set initial HPs\n",
    "        # first for kernel 1 (small variance component)\n",
    "        k = self.covar_module.kernels[0]\n",
    "        k.outputscale = 0.06 # signal var\n",
    "        k.base_kernel.kernels[0].lengthscale =  5.2 # time in hours\n",
    "        k.base_kernel.kernels[1].lengthscale = 17.5 # length in km\n",
    "        \n",
    "        # second for kernel 2\n",
    "        k = self.covar_module.kernels[1]\n",
    "        k.outputscale = 0.21\n",
    "        k.base_kernel.kernels[0].lengthscale = 206.5 # time in hours\n",
    "        k.base_kernel.kernels[1].lengthscale =  71.7 # length in Kilometers\n",
    "            \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneScaleGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, full_GP):\n",
    "        \"\"\"train_x is shape [N, 3], each x[0] is [t, lat, lon]\n",
    "        \"\"\"\n",
    "        \n",
    "        # define likelihood with sum of variances\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        likelihood.noise_covar.raw_noise_constraint.lower_bound = torch.tensor(1e-6)\n",
    "        likelihood.noise = full_GP.likelihood.noise + full_GP.covar_module.kernels[0].outputscale\n",
    "        super(OneScaleGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(active_dims=[0])*\n",
    "            gpytorch.kernels.RBFKernel(active_dims=[1,2])\n",
    "        )\n",
    "        \n",
    "        # first for kernel 1 (small variance component)\n",
    "        full_gp_k0 = full_GP.covar_module.kernels[1]\n",
    "        k = self.covar_module\n",
    "        k.outputscale = full_gp_k0.outputscale\n",
    "        # time in hours\n",
    "        k.base_kernel.kernels[0].lengthscale = full_gp_k0.base_kernel.kernels[0].lengthscale\n",
    "        # length in km\n",
    "        k.base_kernel.kernels[1].lengthscale = full_gp_k0.base_kernel.kernels[1].lengthscale \n",
    "       \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit parameters of a Two-scale GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 001/025 - Loss: -0.924   k1 (scale: 0.06, ls: 5.2, 17.5)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001297\n",
      "Iter 002/025 - Loss: -0.944   k1 (scale: 0.08, ls: 5.0, 17.4)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001281\n",
      "Iter 003/025 - Loss: -0.945   k1 (scale: 0.09, ls: 4.9, 17.1)     k2 (scale: 0.20, ls: 206.5, 71.7) noise: 0.001266\n",
      "Iter 004/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.9)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001204\n",
      "Iter 005/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.7)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 006/025 - Loss: -0.946   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 007/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 008/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 009/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 010/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 011/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 012/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 013/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 014/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 015/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 016/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 017/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 018/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 019/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 020/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 021/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 022/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 023/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 024/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n",
      "Iter 025/025 - Loss: -0.945   k1 (scale: 0.08, ls: 4.9, 15.8)     k2 (scale: 0.21, ls: 206.5, 71.7) noise: 0.001198\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "likelihood.noise_covar.raw_noise_constraint.lower_bound = torch.tensor(1e-6)\n",
    "likelihood.noise = 0.001297 # initialize noise to value found in l\n",
    "two_scale_gp = TwoScaleGPModel(X, Y, likelihood)\n",
    "\n",
    "# set to training mode\n",
    "likelihood.train()\n",
    "two_scale_gp.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.LBFGS([\n",
    "    {'params': two_scale_gp.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "with gpytorch.settings.fast_computations(log_prob=False, solves=False, covar_root_decomposition=False):\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, two_scale_gp)\n",
    "\n",
    "    I = 25\n",
    "    for i in range(I):\n",
    "        optimizer.zero_grad() # Zero gradients from previous iteration\n",
    "        \n",
    "        output = two_scale_gp(X) # Output from model\n",
    "        \n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, Y)\n",
    "        loss.backward()\n",
    "        print('Iter %03d/%03d - Loss: %.3f   k1 (scale: %.2f, ls: %.1f, %.1f) \\\n",
    "    k2 (scale: %.2f, ls: %.1f, %.1f) noise: %.6f' % (\n",
    "            i + 1, I, loss.item(),\n",
    "            two_scale_gp.covar_module.kernels[0].outputscale,\n",
    "            two_scale_gp.covar_module.kernels[0].base_kernel.kernels[0].lengthscale.item(),\n",
    "            two_scale_gp.covar_module.kernels[0].base_kernel.kernels[1].lengthscale.item(),\n",
    "            two_scale_gp.covar_module.kernels[1].outputscale,\n",
    "            two_scale_gp.covar_module.kernels[1].base_kernel.kernels[0].lengthscale.item(),\n",
    "            two_scale_gp.covar_module.kernels[1].base_kernel.kernels[1].lengthscale.item(),\n",
    "            two_scale_gp.likelihood.noise.item()\n",
    "        ))\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = two_scale_gp(X)\n",
    "            loss = -mll(output, Y)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize one-scale GP model\n",
    "one_scale_gp = OneScaleGPModel(X, Y, two_scale_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulate estimates as affine transformations and compute a c-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_A_C_and_Sigma(one_scale_gp, two_scale_gp, X):\n",
    "    # get likelihood and prior cov for two scale model\n",
    "    N = X.shape[0]\n",
    "    K2 = two_scale_gp.covar_module(X).detach().numpy()\n",
    "    Sigma = two_scale_gp.likelihood.noise.detach().numpy()*np.eye(N)\n",
    "    \n",
    "    # get prior cov_for one scale model\n",
    "    K1 = one_scale_gp.covar_module(X).detach().numpy()\n",
    "    print(\"K1 diag, K2 diag\", np.diag(K1)[:3], np.diag(K2)[:3])\n",
    "    K1 += (one_scale_gp.likelihood.noise.detach().numpy() - \n",
    "           two_scale_gp.likelihood.noise.detach().numpy())*np.eye(N)\n",
    "    print(\"K1 diag, K2 diag\", np.diag(K1)[:3], np.diag(K2)[:3])\n",
    "    \n",
    "    A = np.linalg.inv(np.eye(N) + Sigma@np.linalg.inv(K1))\n",
    "    C = np.linalg.inv(np.eye(N) + Sigma@np.linalg.inv(K2))\n",
    "    return A, C, Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  400\n",
      "K1 diag, K2 diag [0.21256207 0.21256207 0.21256207] [0.29136112 0.29136112 0.29136112]\n",
      "K1 diag, K2 diag [0.29136112 0.29136112 0.29136112] [0.29136112 0.29136112 0.29136112]\n",
      "N :  400\n",
      "c_val latitudinal \t:  0.9998049269934458\n",
      "c_val longitudinal \t:  0.9987361748360667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briantrippe/anaconda/lib/python3.6/site-packages/scipy/optimize/zeros.py:550: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  r = _zeros._bisect(f, a, b, xtol, rtol, maxiter, args, full_output, disp)\n"
     ]
    }
   ],
   "source": [
    "# Load a smaller subset of the data for computing a c-value as in the paper\n",
    "np.random.seed(42)\n",
    "df_more_drifters = subset_data(\n",
    "    df_full, lat_min=lat_min, lat_max=lat_max, lon_min=lon_min, lon_max=lon_max, \n",
    "    t_min=400, t_max=424,\n",
    "    downsample_freq=12,\n",
    "    n_drifters=50)\n",
    "\n",
    "### Pull out covariates \n",
    "X_small = df_more_drifters[['hour', 'x','y']].to_numpy()\n",
    "X_small = torch.tensor(X_small, dtype=torch.float)\n",
    "\n",
    "### compute c_values for latitudinal and longitudinal components separately\n",
    "Y_small_lat = df_more_drifters['U'].to_numpy()\n",
    "Y_small_lon = df_more_drifters['V'].to_numpy()\n",
    "\n",
    "A, C, Sigma = get_A_C_and_Sigma(one_scale_gp, two_scale_gp, X_small)\n",
    "N = A.shape[0]\n",
    "print(\"N : \", N)\n",
    "c_val_lat = affine_ops.c_value(Y_small_lat, A, np.zeros(N), C, np.zeros(N), Sigma)\n",
    "print(\"c_val latitudinal \\t: \", c_val_lat)\n",
    "\n",
    "# Additionally compute a c-value for an estimate of the longitudinal velocity component\n",
    "# We model this second component independently, as in prior work.\n",
    "c_val_lon = affine_ops.c_value(Y_small_lon, A, np.zeros(N), C, np.zeros(N), Sigma)\n",
    "print(\"c_val longitudinal \\t: \", c_val_lon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
